{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppf_bis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Yy0WHpW3X9oX",
        "9SwIjOX9frk6",
        "ZjxFdZTXBZWb",
        "AabTJc6dflSy",
        "DosPYRjLfcoB",
        "5gHJ4YRbnmWJ",
        "aBusNmsKpc2Z",
        "NwwLDouezDLc",
        "VpMowosc0Bzf"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaldb/examples/blob/master/eeg_mac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy0WHpW3X9oX"
      },
      "source": [
        "# 뇌파로부터 마취제 농도 예측 인공지능 모델 실습\n",
        "Sevoflurane 마취 중 뇌파로부터 마취제 농도(age related MAC) 예측 모델\n",
        "\n",
        "## VitalDB 데이터 셋 이용\n",
        "본 예제에서는 오픈 생체 신호 데이터셋인 VitalDB를 이용하는 모든 사용자는 반드시 아래 Data Use Agreement에 동의하여야 합니다.\n",
        "\n",
        "https://vitaldb.net/data-bank/?query=guide&documentId=13qqajnNZzkN7NZ9aXnaQ-47NWy7kx-a6gbrcEsi-gak&sectionId=h.usmoena3l4rb\n",
        "\n",
        "동의하지 않을 경우 이 창을 닫으세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SwIjOX9frk6"
      },
      "source": [
        "## 본 프로그램에서 이용할 라이브러리 설치 및 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch6czkFZfw_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82d5047-95b9-4501-f889-efa5aa1c18e2"
      },
      "source": [
        "!pip install vitaldb\n",
        "\n",
        "import vitaldb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import scipy.signal\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vitaldb\n",
            "  Downloading vitaldb-0.0.11-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from vitaldb) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vitaldb) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from vitaldb) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->vitaldb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->vitaldb) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->vitaldb) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vitaldb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vitaldb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vitaldb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vitaldb) (2021.5.30)\n",
            "Installing collected packages: vitaldb\n",
            "Successfully installed vitaldb-0.0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjxFdZTXBZWb"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AabTJc6dflSy"
      },
      "source": [
        "VitalDB Web API를 통해 데이터 로딩\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b_TyfelWg6e"
      },
      "source": [
        "cachefile = '{}sec_{}cases.npz'.format(SEGLEN // SRATE, MAX_CASES)\n",
        "if os.path.exists(cachefile):\n",
        "    dat = np.load(cachefile)\n",
        "    x, y, b, c = dat['x'], dat['y'], dat['b'], dat['c']\n",
        "else:\n",
        "    df_trks = pd.read_csv(\"https://api.vitaldb.net/trks\")  # 트랙 정보\n",
        "    df_cases = pd.read_csv(\"https://api.vitaldb.net/cases\")  # 환자 정보\n",
        "\n",
        "    # 데이터 로딩 시 컬럼 순서\n",
        "    EEG = 0\n",
        "    SEVO = 1\n",
        "    BIS = 2\n",
        "\n",
        "    # inclusion & exclusion criteria\n",
        "    caseids = set(df_cases.loc[df_cases['age'] > 18, 'caseid']) &\\\n",
        "        set(df_trks.loc[df_trks['tname'] == 'BIS/EEG1_WAV', 'caseid']) &\\\n",
        "        set(df_trks.loc[df_trks['tname'] == 'BIS/BIS', 'caseid']) &\\\n",
        "        set(df_trks.loc[df_trks['tname'] == 'Primus/EXP_SEVO', 'caseid'])\n",
        "\n",
        "    x = []  \n",
        "    y = []  # sevo\n",
        "    b = []  # bis\n",
        "    c = []  # caseids\n",
        "    icase = 0  # 현재까지 로딩된 케이스 수\n",
        "    for caseid in caseids:\n",
        "        print('loading {} ({}/{})'.format(caseid, icase, MAX_CASES), end='...', flush=True)\n",
        "\n",
        "        # 아래 값들이 있으면 제외\n",
        "        if np.any(vitaldb.load_case(caseid, 'Orchestra/PPF20_CE') > 0.2):\n",
        "            print('propofol')\n",
        "            continue\n",
        "        if np.any(vitaldb.load_case(caseid, 'Primus/EXP_DES') > 1):\n",
        "            print('desflurane')\n",
        "            continue\n",
        "        if np.any(vitaldb.load_case(caseid, 'Primus/FEN2O') > 2):\n",
        "            print('n2o')\n",
        "            continue\n",
        "        if np.any(vitaldb.load_case(caseid, 'Orchestra/RFTN50_CE') > 0.2):\n",
        "            print('remifentanil')\n",
        "            continue\n",
        "\n",
        "        # extract data\n",
        "        vals = vitaldb.load_case(caseid, ['BIS/EEG1_WAV', 'Primus/EXP_SEVO', 'BIS/BIS'], 1 / SRATE)\n",
        "        if np.nanmax(vals[:, SEVO]) < 1:\n",
        "            print('all sevo <= 1')\n",
        "            continue\n",
        "\n",
        "        # convert etsevo to the age related mac\n",
        "        age = df_cases.loc[df_cases['caseid'] == caseid, 'age'].values[0]\n",
        "        vals[:, SEVO] /= 1.80 * 10 ** (-0.00269 * (age - 40))\n",
        "\n",
        "        if not np.any(vals[:, BIS] > 0):\n",
        "            print('all bis <= 0')\n",
        "            continue\n",
        "\n",
        "        # 뇌파가 잘 나와야 하기 때문에 bis가 잘 나온 곳 부터 시작함\n",
        "        valid_bis_idx = np.where(vals[:, BIS] > 0)[0]\n",
        "        first_bis_idx = valid_bis_idx[0]\n",
        "        last_bis_idx = valid_bis_idx[-1]\n",
        "        vals = vals[first_bis_idx:last_bis_idx + 1, :]\n",
        "\n",
        "        if len(vals) < 1800 * SRATE:  # 30분 이하인 case는 사용하지 않음\n",
        "            print('{} len < 30 min'.format(caseid))\n",
        "            continue\n",
        "\n",
        "        # MAC 값과 BIS 값은 5초까지 뒤로 채움\n",
        "        vals[:, SEVO:] = pd.DataFrame(vals[:, SEVO:]).ffill(limit=5 * SRATE).values\n",
        "\n",
        "        # 1초 간격으로 데이터 추출\n",
        "        # case 시작 부터 종료까지 dataset 에 넣음\n",
        "        oldlen = len(y)\n",
        "        for irow in range(SEGLEN, len(vals), SRATE):\n",
        "            bis = vals[irow, BIS]\n",
        "            mac = vals[irow, SEVO]\n",
        "            if np.isnan(bis) or np.isnan(mac) or bis == 0:\n",
        "                continue\n",
        "            # dataset 에 추가\n",
        "            eeg = vals[irow - SEGLEN:irow, EEG]\n",
        "            x.append(eeg)\n",
        "            y.append(mac)\n",
        "            b.append(bis)\n",
        "            c.append(caseid)\n",
        "\n",
        "        # valid case\n",
        "        icase += 1\n",
        "        print('{} samples read -> total {} samples ({}/{})'.format(len(y) - oldlen, len(y), icase, MAX_CASES))\n",
        "        if icase >= MAX_CASES:\n",
        "            break\n",
        "\n",
        "    # 입력 데이터셋을 numpy array로 변경\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    b = np.array(b)\n",
        "    c = np.array(c)\n",
        "\n",
        "    # save cahce file\n",
        "    np.savez(cachefile, x=x, y=y, b=b, c=c)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwwLDouezDLc"
      },
      "source": [
        "## 데이터셋 포맷 및 차원 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHa2fte0zGEv"
      },
      "source": [
        "# 결측값이 있으면 제거\n",
        "print('invalid samples...', end='', flush=True)\n",
        "valid_mask = ~(np.max(np.isnan(x), axis=1) > 0) # nan이 있으면 제거\n",
        "valid_mask &= (np.max(x, axis=1) - np.min(x, axis=1) > 12)  # bis 임피던스 체크 eeg의 전체 range가 12 미만이면 제거\n",
        "x = x[valid_mask, :]\n",
        "y = y[valid_mask]\n",
        "b = b[valid_mask]\n",
        "c = c[valid_mask]\n",
        "print('{:.1f}% removed'.format(100*(1-np.mean(valid_mask))))\n",
        "\n",
        "# 필터링\n",
        "print('baseline drift...', end='', flush=True)\n",
        "x -= scipy.signal.savgol_filter(x, 91, 3)  # remove baseline drift\n",
        "print('removed')\n",
        "\n",
        "# noise 가 많으면 제거\n",
        "print('noisy samples...', end='', flush=True)\n",
        "valid_mask = (np.nanmax(np.abs(x), axis=1) < 100) # noisy sample \n",
        "\n",
        "x = x[valid_mask, :]  # CNN 에 넣기 위해서는 3차원이어야 한다. 마지막 차원을 추가\n",
        "y = y[valid_mask]\n",
        "b = b[valid_mask]\n",
        "c = c[valid_mask]\n",
        "print('{:.1f}% removed'.format(100*(1-np.mean(valid_mask))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpMowosc0Bzf"
      },
      "source": [
        "## 데이터를 학습(train)과 테스트(test)로 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjdvjsJ10JAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dedf00e9-423c-4ad4-ab83-2f22b254f1be"
      },
      "source": [
        "# 최종적으로 로딩 된 caseid\n",
        "caseids = list(np.unique(c))\n",
        "random.shuffle(caseids)\n",
        "\n",
        "# case 단위로 train, test case로 나눔\n",
        "ntest = max(1, int(len(caseids) * 0.2))\n",
        "caseids_train = caseids[ntest:]\n",
        "caseids_test = caseids[:ntest]\n",
        "\n",
        "train_mask = np.isin(c, caseids_train)\n",
        "test_mask = np.isin(c, caseids_test)\n",
        "x_train = x[train_mask]\n",
        "y_train = y[train_mask]\n",
        "x_test = x[test_mask]\n",
        "y_test = y[test_mask]\n",
        "b_test = b[test_mask]\n",
        "c_test = c[test_mask]\n",
        "\n",
        "print('====================================================')\n",
        "print('total: {} cases {} samples'.format(len(caseids), len(y)))\n",
        "print('train: {} cases {} samples'.format(len(np.unique(c[train_mask])), len(y_train)))\n",
        "print('test {} cases {} samples'.format(len(np.unique(c_test)), len(y_test)))\n",
        "print('====================================================')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 45 cases 53222 samples, testing: 5 cases 5266 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPi6FIbB0NkF"
      },
      "source": [
        "# Model building\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uytYU2Fu0rPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c66ef36-c08e-4e18-8f25-6d6d3e912bef"
      },
      "source": [
        "import keras.models\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Layer, LayerNormalization, Dense, Dropout, Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Input, concatenate, multiply, dot, MultiHeadAttention\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# hyperparameters\n",
        "tests = {\n",
        "    \"nfilt\" : [16, 32, 64],\n",
        "    \"fnode\" : [32, 64, 128],\n",
        "    \"clayer\" : [1, 2, 3, 4],\n",
        "    \"droprate\" : [0.1, 0.2],\n",
        "    \"filtsize\" : [5, 7, 9, 11],\n",
        "    'poolsize' : [2, 4, 8],\n",
        "    \"pooltype\" : ['avg', 'max']\n",
        "}\n",
        "\n",
        "# https://keras.io/examples/nlp/text_classification_with_transformer/\n",
        "keys, values = zip(*tests.items())\n",
        "permutations_dicts = it.product(*values)\n",
        "permutations_dicts = list(permutations_dicts)\n",
        "random.shuffle(permutations_dicts)\n",
        "for nfilt, fnode, clayer, droprate, filtsize, poolsize, pooltype in permutations_dicts:\n",
        "\n",
        "    keras.backend.clear_session()\n",
        "    \n",
        "    odir = '{}cases_{}sec'.format(MAX_CASES, SEGLEN // SRATE)\n",
        "    odir += '_cnn{} filt{} size{} pool{} {} do{}'.format(clayer, nfilt, filtsize, poolsize, pooltype, droprate)\n",
        "    print(\"============================\")\n",
        "    print(odir)\n",
        "    print(\"============================\")\n",
        "\n",
        "    out = inp = Input(shape=(x_train.shape[1], 1))\n",
        "    out = Conv1D(filters=nfilt, kernel_size=filtsize, padding='same')(out)\n",
        "    # conv 여러층    \n",
        "    for i in range(clayer):\n",
        "        out = Conv1D(filters=nfilt, kernel_size=filtsize, padding='same', activation='relu')(out)\n",
        "        out = MaxPooling1D(poolsize, padding='same')(out)\n",
        "    if pooltype == \"avg\":\n",
        "        out = GlobalAveragePooling1D()(out)\n",
        "    else:\n",
        "        out = GlobalMaxPooling1D()(out)\n",
        "\n",
        "    if droprate:\n",
        "        out = Dropout(droprate)(out)\n",
        "    out = Dense(fnode)(out)\n",
        "    if droprate:\n",
        "        out = Dropout(droprate)(out)\n",
        "    out = Dense(1)(out)\n",
        "\n",
        "    if not os.path.exists(odir):\n",
        "        os.mkdir(odir)\n",
        "\n",
        "    cache_path = odir + \"/weights.hdf5\"\n",
        "    model = Model(inputs=[inp], outputs=[out])\n",
        "    model.summary()\n",
        "    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "    hist = model.fit(x_train[..., None], y_train, validation_split=0.2, epochs=10, batch_size=BATCH_SIZE,\n",
        "                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=cache_path, verbose=1, save_best_only=True),\n",
        "                               EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto'),\n",
        "                               ])\n",
        "\n",
        "    # prediction\n",
        "    pred_test = model.predict(x_test[..., None], batch_size=BATCH_SIZE).flatten()\n",
        "\n",
        "    # 성능을 계산하여 출력\n",
        "    test_mae = np.mean(np.abs(y_test - pred_test))\n",
        "    for caseid in np.unique(c_test):\n",
        "        case_mask = (c_test == caseid)\n",
        "        pred_test[case_mask] = scipy.signal.medfilt(pred_test[case_mask], 31)\n",
        "\n",
        "    # prediction\n",
        "    for caseid in np.unique(c_test):\n",
        "        case_mask = (c_test == caseid)\n",
        "        case_len = np.sum(case_mask)\n",
        "        if case_len == 0:\n",
        "            continue\n",
        "\n",
        "        our_mae = np.mean(np.abs(y_test[case_mask] - pred_test[case_mask]))\n",
        "        print('Total MAE={:.4f}, CaseID {}, MAE={:.4f}'.format(test_mae, caseid, our_mae))\n",
        "\n",
        "        t = np.arange(0, case_len)\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        plt.plot(t, y_test[case_mask], label='MAC')  # 측정 결과 \n",
        "        plt.plot(t, pred_test[case_mask], label='Ours ({:.4f})'.format(our_mae))\n",
        "        plt.legend(loc=\"upper left\")\n",
        "        plt.tight_layout()\n",
        "        plt.xlim([0, case_len])\n",
        "        plt.ylim([0, 2])\n",
        "        plt.savefig('{}/{:.4f}_{}.png'.format(odir, our_mae, caseid))\n",
        "        plt.close()\n",
        "\n",
        "    # 최종 디렉토리 이름 바꿈\n",
        "    os.rename(odir, '{:.4f}'.format(test_mae) + odir + '_' + str(random.randint(0, 1000)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - 19s 110ms/step - loss: 0.1017 - mean_absolute_percentage_error: 23.7280 - val_loss: 0.1096 - val_mean_absolute_percentage_error: 30.3796\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 0.0933 - mean_absolute_percentage_error: 21.3791 - val_loss: 0.1070 - val_mean_absolute_percentage_error: 29.2109\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 0.0877 - mean_absolute_percentage_error: 20.2326 - val_loss: 0.0990 - val_mean_absolute_percentage_error: 28.2095\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 0.0830 - mean_absolute_percentage_error: 19.7110 - val_loss: 0.0982 - val_mean_absolute_percentage_error: 28.2541\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 0.0815 - mean_absolute_percentage_error: 19.4846 - val_loss: 0.0978 - val_mean_absolute_percentage_error: 28.3904\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 0.0809 - mean_absolute_percentage_error: 19.3659 - val_loss: 0.0948 - val_mean_absolute_percentage_error: 27.6144\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 0.0803 - mean_absolute_percentage_error: 19.2700 - val_loss: 0.0983 - val_mean_absolute_percentage_error: 28.3905\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 0.0804 - mean_absolute_percentage_error: 19.2611 - val_loss: 0.0950 - val_mean_absolute_percentage_error: 27.9300\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 0.0800 - mean_absolute_percentage_error: 19.1936 - val_loss: 0.0981 - val_mean_absolute_percentage_error: 28.8082\n"
          ]
        }
      ]
    }
  ]
}