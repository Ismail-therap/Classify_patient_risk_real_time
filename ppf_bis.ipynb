{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppf_bis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaldb/examples/blob/master/ppf_bis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy0WHpW3X9oX",
        "colab_type": "text"
      },
      "source": [
        "# 프로포폴, 레미펜타닐 주입 속도로부터 BIS 예측\n",
        "\n",
        "## VitalDB 데이터 셋 이용\n",
        "본 예제에서는 오픈 생체 신호 데이터셋인 VitalDB를 이용하는 모든 사용자는 반드시 아래 Data Use Agreement에 동의하여야 합니다.\n",
        "\n",
        "https://vitaldb.net/data-bank/?query=guide&documentId=13qqajnNZzkN7NZ9aXnaQ-47NWy7kx-a6gbrcEsi-gak&sectionId=h.usmoena3l4rb\n",
        "\n",
        "동의하지 않을 경우 이 창을 닫으세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SwIjOX9frk6",
        "colab_type": "text"
      },
      "source": [
        "## 본 프로그램에서 이용할 라이브러리 및 옵션들"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch6czkFZfw_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "c4508e0e-7444-4044-c4ad-84e424fd1ac8"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/vitaldb/vitalutils/master/python/vitaldb.py\n",
        "\n",
        "import vitaldb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "LSTM_TIMEPOINTS = 180\n",
        "LSTM_NODES = 6\n",
        "FNN_NODES = 16\n",
        "BATCH_SIZE = 256\n",
        "MAX_CASES = 100  # 본 예제에서 사용할 최대 case 수"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-12 00:18:32--  https://raw.githubusercontent.com/vitaldb/vitalutils/master/python/vitaldb.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15107 (15K) [text/plain]\n",
            "Saving to: ‘vitaldb.py’\n",
            "\n",
            "vitaldb.py          100%[===================>]  14.75K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-09-12 00:18:32 (1.47 MB/s) - ‘vitaldb.py’ saved [15107/15107]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjxFdZTXBZWb",
        "colab_type": "text"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AabTJc6dflSy",
        "colab_type": "text"
      },
      "source": [
        "## 트랙 데이터 가져오기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qsz5TKVf6Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_trks = pd.read_csv('https://api.vitaldb.net/v2/trks')\n",
        "ppf_cases = set(df_trks[df_trks['tname'] == 'Orchestra/PPF20_VOL']['caseid'])  # print(len(ppf_cases))  # 3523\n",
        "rft_cases = set(df_trks[df_trks['tname'] == 'Orchestra/RFTN20_VOL']['caseid'])  # print(len(rft_cases))  # 4791\n",
        "tiva_cases = ppf_cases & rft_cases  # print(len(tiva_cases))  # 3457\n",
        "bis_cases = set(df_trks[df_trks['tname'] == 'BIS/BIS']['caseid'])  # print(len(bis_cases))  # 5867\n",
        "study_cases = sorted(list(tiva_cases & bis_cases))  # print(len(study_cases))  # 3289"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DosPYRjLfcoB",
        "colab_type": "text"
      },
      "source": [
        "## 임상 데이터 가져오기 (나이, 성별, 키, 몸무게)\n",
        "- clinical informaiton을 df_cases에 읽어오기\n",
        "- 사용할 age, sex, weight, height 가져오기\n",
        "- 성별이 F(=female)은 0으로, M(=male)은 1로 치환\n",
        "- 18세 이상, 몸무게 35이상 기준으로 case 준비(inclusion, exclusion criteria)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw8dXeGXfg0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cases = pd.read_csv(\"https://api.vitaldb.net/cases\")\n",
        "cases = df_cases.loc[[caseid in study_cases for caseid in df_cases['caseid']], ['caseid', 'age', 'sex', 'weight', 'height']]\n",
        "cases['sex'] = cases['sex'] == 'F'\n",
        "cases = cases[cases['age'] > 18]  \n",
        "cases = cases[cases['weight'] > 35]  \n",
        "cases = cases[1:MAX_CASES]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gHJ4YRbnmWJ",
        "colab_type": "text"
      },
      "source": [
        "## 데이터셋 준비\n",
        "- 전체 case id 모으고(caseids)\n",
        "- caseid별로 나이, 성별, 키, 몸무게 담기(caseid_aswh)\n",
        "- 최대로 로딩 가능한 case 수 알기(ncase) \n",
        "- 그리고 caseid를 무작위로 섞음(.shuffle)\n",
        "- dataset을 담을 변수를 설정(x_ppf, x_rrf, x_aswh, x_caseid, y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n2o5_2QoFy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "caseids = cases.values[:,0]\n",
        "caseid_aswh = {row[0]: row[1:].astype(float) for row in cases.values}\n",
        "ncase = min(MAX_CASES, len(caseids))\n",
        "np.random.shuffle(caseids)\n",
        "\n",
        "# vital 파일로부터 dataset 을 만듬\n",
        "x_ppf = []  # 각 레코드의 프로포폴 주입량\n",
        "x_rft = []  # 각 레코드의 레미펜타닐 주입량\n",
        "x_aswh = []  # 각 레코드의 나이, 성별, 키, 몸무게\n",
        "x_caseid = []  # 각 레코드의 caseid\n",
        "y = []  # 각 레코드의 출력값 (bis)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBusNmsKpc2Z",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 전처리\n",
        "- propofol(ppf20), remifentanyl(rftn20), bis를 로딩한다\n",
        "- 기록된 데이터가 짧거나, drug infusion이 실제 없었던 케이스, bis값이 적절하지 않은 케이스는 거르기\n",
        "- 결측값 및 음수 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6CHJpySrDxG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6bdb02f9-f452-4fc6-c43f-07308ced35aa"
      },
      "source": [
        "PPF_DOSE = 0\n",
        "RFT_DOSE = 1\n",
        "BIS = 2\n",
        "icase = 0\n",
        "\n",
        "for caseid in caseids:\n",
        "    ppf20_tid = df_trks[(df_trks['caseid'] == caseid) & (df_trks['tname'] == 'Orchestra/PPF20_VOL')]['tid'].values[0]\n",
        "    rftn20_tid = df_trks[(df_trks['caseid'] == caseid) & (df_trks['tname'] == 'Orchestra/RFTN20_VOL')]['tid'].values[0]\n",
        "    bis_tid = df_trks[(df_trks['caseid'] == caseid) & (df_trks['tname'] == 'BIS/BIS')]['tid'].values[0]\n",
        "    vals = vitaldb.load_trks([ppf20_tid, rftn20_tid, bis_tid], 10) #10s 간격 추출\n",
        "\n",
        "    # 2시간 이내의 case 들은 사용하지 않음, 720 =  2hr * 60min/hr * 6개/min\n",
        "    if vals.shape[0] < 720:\n",
        "        continue\n",
        "\n",
        "    # 결측값은 측정된 마지막 값으로 대체\n",
        "    vals = pd.DataFrame(vals).fillna(method='ffill').values\n",
        "    vals = np.nan_to_num(vals, 0)  # 맨 앞 쪽 결측값은 0으로 대체\n",
        "\n",
        "    # drug 주입을 하지 않은 경우 혹은 bis를 켜지 않은 경우 사용하지 않음\n",
        "    if (np.max(vals, axis=0) <= 1).any():\n",
        "        continue\n",
        "\n",
        "    # drug infusion 시작 시간을 구하고 그 이전을 삭제\n",
        "    first_ppf_idx = np.where(vals[:, PPF_DOSE] > 1)[0][0]\n",
        "    first_rft_idx = np.where(vals[:, RFT_DOSE] > 1)[0][0]\n",
        "    first_drug_idx = min(first_ppf_idx, first_rft_idx)\n",
        "    vals = vals[first_drug_idx:, :]\n",
        "\n",
        "    # volume 을 rate로 변경\n",
        "    vals[1:, PPF_DOSE] -= vals[:-1, PPF_DOSE]\n",
        "    vals[1:, RFT_DOSE] -= vals[:-1, RFT_DOSE]\n",
        "    vals[0, PPF_DOSE] = 0\n",
        "    vals[0, RFT_DOSE] = 0\n",
        "\n",
        "    # 음수 값(volume 감소)을 0으로 대체\n",
        "    vals[vals < 0] = 0\n",
        "\n",
        "    # 유효한 bis 값들을 가져옴\n",
        "    valid_bis_idx = np.where(vals[:, BIS] > 0)[0]\n",
        "\n",
        "    # 2시간 이내의 case들은 사용하지 않음\n",
        "    if valid_bis_idx.shape[0] < 720:\n",
        "        continue\n",
        "\n",
        "    # bis 값의 첫 값이 80 이하이거나 마지막 값이 70 이하인 case는 사용하지 않음\n",
        "    first_bis_idx = valid_bis_idx[0]\n",
        "    last_bis_idx = valid_bis_idx[-1]\n",
        "    if vals[first_bis_idx, BIS] < 80 or vals[last_bis_idx, BIS] < 70:\n",
        "        continue\n",
        "\n",
        "    icase += 1\n",
        "    print('loading ({}/{}): caseid {:.0f}, first bis {:.1f}, last bis {:.1f}'.format(icase, ncase, caseid, vals[first_bis_idx, BIS], vals[last_bis_idx, BIS]))\n",
        "\n",
        "    # infusion 시작 전 LSTM_TIMEPOINTS 동안의 dose와 bis를 모두 0으로 세팅\n",
        "    vals = np.vstack((np.zeros((LSTM_TIMEPOINTS - 1, 3)), vals))\n",
        "\n",
        "    # 현 case의 나이, 성별, 키, 몸무게를 가져옴\n",
        "    aswh = caseid_aswh[caseid]\n",
        "\n",
        "    # case 시작 부터 종료 까지 dataset 에 넣음\n",
        "    for irow in range(1, vals.shape[0] - LSTM_TIMEPOINTS - 1):\n",
        "        bis = vals[irow + LSTM_TIMEPOINTS, BIS]\n",
        "        if bis == 0:\n",
        "            continue\n",
        "\n",
        "        # 데이터셋에 입력값을 넣음\n",
        "        x_ppf.append(vals[irow:irow + LSTM_TIMEPOINTS, PPF_DOSE])\n",
        "        x_rft.append(vals[irow:irow + LSTM_TIMEPOINTS, RFT_DOSE])\n",
        "        x_aswh.append(aswh)\n",
        "        x_caseid.append(caseid)\n",
        "        y.append(bis)\n",
        "\n",
        "    if icase >= ncase:\n",
        "        break\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading (1/9): caseid 13, first bis 95.8, last bis 79.7\n",
            "loading (2/9): caseid 16, first bis 91.4, last bis 80.5\n",
            "loading (3/9): caseid 19, first bis 97.6, last bis 94.7\n",
            "loading (4/9): caseid 17, first bis 91.6, last bis 86.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwwLDouezDLc",
        "colab_type": "text"
      },
      "source": [
        "## 데이터셋 포맷 및 차원 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHa2fte0zGEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모든 케이스 확인, xppf... y까지 담은후,\n",
        "# 입력 데이터셋을 numpy array로 변경\n",
        "\n",
        "x_ppf = np.array(x_ppf)[..., None]  # LSTM 에 넣기 위해서는 3차원이어야 한다. 마지막 차원을 추가\n",
        "x_rft = np.array(x_rft)[..., None]\n",
        "x_aswh = np.array(x_aswh)\n",
        "y = np.array(y)\n",
        "x_caseid = np.array(x_caseid)\n",
        "\n",
        "# 최종적으로 로딩 된 caseid\n",
        "caseids = np.unique(x_caseid)\n",
        "\n",
        "# normalize data\n",
        "x_aswh = (x_aswh - np.mean(x_aswh, axis=0)) / np.std(x_aswh, axis=0)\n",
        "\n",
        "# bis 값은 최대값이 98 이므로 98로 나눠 normalization\n",
        "y /= 98\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpMowosc0Bzf",
        "colab_type": "text"
      },
      "source": [
        "## 데이터를 학습(train)과 테스트(test)로 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjdvjsJ10JAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c80153be-5901-4c0d-d654-c14f28b07615"
      },
      "source": [
        "# train, test case로 나눔\n",
        "ntest = int(ncase * 0.1)\n",
        "nval = int(ncase * 0.1)\n",
        "ntrain = ncase - nval - ntest\n",
        "train_caseids = caseids[:ntrain]\n",
        "val_caseids = caseids[ntrain:ntrain + nval]\n",
        "test_caseids = caseids[ncase - ntest:ncase]\n",
        "\n",
        "# train set과 test set 으로 나눔\n",
        "train_mask = np.array([caseid in train_caseids for caseid in x_caseid])\n",
        "val_mask = np.array([caseid in val_caseids for caseid in x_caseid])\n",
        "test_mask = np.array([caseid in test_caseids for caseid in x_caseid])\n",
        "x_train = [x_ppf[train_mask], x_rft[train_mask], x_aswh[train_mask]]\n",
        "y_train = y[train_mask]\n",
        "x_val = [x_ppf[val_mask], x_rft[val_mask], x_aswh[val_mask]]\n",
        "y_val = y[val_mask]\n",
        "x_test = [x_ppf[test_mask], x_rft[test_mask], x_aswh[test_mask]]\n",
        "y_test = y[test_mask]\n",
        "\n",
        "print('train: {} cases {} samples'.format(len(train_caseids), np.sum(train_mask)))\n",
        "print('val: {} cases {} samples'.format(len(val_caseids), np.sum(val_mask)))\n",
        "print('test: {} cases {} samples'.format(len(test_caseids), np.sum(test_mask)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 4 cases 6641 samples\n",
            "val: 0 cases 0 samples\n",
            "test: 0 cases 0 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPi6FIbB0NkF",
        "colab_type": "text"
      },
      "source": [
        "# Model building\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqaAFBgJ0gdq",
        "colab_type": "text"
      },
      "source": [
        "## Keras 라이브러리 로딩 및 출력 폴더 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uytYU2Fu0rPi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34c5f77b-d9df-4e08-e248-81cdf99b0b8c"
      },
      "source": [
        "from keras.models import Model, load_model\n",
        "from keras.layers import Dense, Dropout, LSTM, Input, concatenate\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# 모델 설계\n",
        "input_cov = Input(batch_shape=(None, 4))\n",
        "input_ppf = Input(batch_shape=(None, LSTM_TIMEPOINTS, 1))\n",
        "input_rft = Input(batch_shape=(None, LSTM_TIMEPOINTS, 1))\n",
        "output_ppf = LSTM(LSTM_NODES, input_shape=(LSTM_TIMEPOINTS, 1), activation='tanh')(input_ppf)\n",
        "output_rft = LSTM(LSTM_NODES, input_shape=(LSTM_TIMEPOINTS, 1), activation='tanh')(input_rft)\n",
        "output = concatenate([output_ppf, output_rft, input_cov])\n",
        "output = Dense(FNN_NODES, activation='relu')(output)\n",
        "output = Dropout(0.2)(output)\n",
        "output = Dense(1, activation='sigmoid')(output)\n",
        "\n",
        "# 학습 데이터의 일부를 지속적으로 공급해 줄 함수\n",
        "def gen():\n",
        "    global BATCH_SIZE, x_ppf, x_rft, x_aswh, y, train_mask\n",
        "    train_idx = np.where(train_mask)[0]\n",
        "    while True:\n",
        "        batch_idx = np.random.choice(train_idx, BATCH_SIZE)\n",
        "        yield [np.take(x_ppf, batch_idx, axis=0), np.take(x_rft, batch_idx, axis=0), np.take(x_aswh, batch_idx, axis=0)], [np.take(y, batch_idx, axis=0)]\n",
        "\n",
        "model = Model(inputs=[input_ppf, input_rft, input_cov], outputs=[output])\n",
        "model.compile(loss=tf.keras.losses.MeanAbsolutePercentageError(), optimizer='adam')\n",
        "hist = model.fit_generator(gen(), validation_data=[x_val, y_val], epochs=100, steps_per_epoch=100,\n",
        "                           callbacks=[EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 41.1593WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 40.9502\n",
            "Epoch 2/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 19.5589WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 19.5451\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 18.5655WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 18.5655\n",
            "Epoch 4/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 17.9909WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 17.9851\n",
            "Epoch 5/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 17.5061WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 17.4858\n",
            "Epoch 6/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 17.1605WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 17.1511\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 16.9224WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 16.9224\n",
            "Epoch 8/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 16.3154WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 16.3092\n",
            "Epoch 9/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 16.0998WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 16.1012\n",
            "Epoch 10/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 16.1723WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 16.1710\n",
            "Epoch 11/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 15.6958WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 15.6987\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 15.5421WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 15.5421\n",
            "Epoch 13/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 15.3542WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 15.3995\n",
            "Epoch 14/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 15.5383WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 15.5389\n",
            "Epoch 15/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 14.8124WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 14.8339\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 14.9772WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 14.9772\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 15.3186WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 15.3186\n",
            "Epoch 18/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 15.1781WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 15.1764\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 15.0283WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 15.0283\n",
            "Epoch 20/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 14.4106WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 14.4038\n",
            "Epoch 21/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 14.0183WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 14.0178\n",
            "Epoch 22/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 14.0969WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 14.0918\n",
            "Epoch 23/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 13.8986WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 13.9255\n",
            "Epoch 24/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 13.9787WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 13.9787\n",
            "Epoch 25/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 14.0924WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 14.0774\n",
            "Epoch 26/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 14.1153WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 14.1068\n",
            "Epoch 27/100\n",
            " 36/100 [=========>....................] - ETA: 0s - loss: 13.9533"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-96e9313feea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanAbsolutePercentageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m hist = model.fit_generator(gen(), validation_data=[x_val, y_val], epochs=100, steps_per_epoch=100,\n\u001b[0;32m---> 28\u001b[0;31m                            callbacks=[EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeVpuglt1W0k",
        "colab_type": "text"
      },
      "source": [
        "## 모델 성능 평가 및 그림 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNA1cUY71cxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 출력 폴더를 생성\n",
        "import os\n",
        "odir = 'output'\n",
        "if not os.path.exists(odir):\n",
        "    os.mkdir(odir)\n",
        "\n",
        "y_pred = model.predict(x_test).flatten()\n",
        "test_err = y_pred - y_test\n",
        "test_rmse = np.mean(np.square(test_err)) ** 0.5\n",
        "\n",
        "print(\"Test RMSE: {}\".format(test_rmse))\n",
        "\n",
        "# 각 case에서 예측 결과를 그림으로 확인\n",
        "for caseid in test_caseids:\n",
        "    case_mask = (x_caseid[test_mask] == caseid)\n",
        "    case_len = np.sum(case_mask)\n",
        "    if case_len == 0:\n",
        "        continue\n",
        "\n",
        "    case_rmse = np.mean(np.square(test_err[case_mask])) ** 0.5\n",
        "    print('{}\\t{}'.format(caseid, case_rmse))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "    t = np.arange(0, case_len)\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    plt.plot(t, y_test[case_mask], t, y_pred[case_mask])\n",
        "    plt.xlim([0, case_len])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.savefig('{}/{:.3f}_{}.png'.format(odir, case_rmse, caseid))\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}